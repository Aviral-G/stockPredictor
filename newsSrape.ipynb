{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mamin\\Desktop\\StockMl\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#importing neccessary libraries \n",
    "\n",
    "#web scraping libraries\n",
    "from newspaper import Article\n",
    "import newspaper\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "\n",
    "#gettting the date\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "#sumarizer libraries\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "\n",
    "#sentiment score analysis. \n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scraper():\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.error_counter = 0\n",
    "        self.df = None\n",
    "    \n",
    "    def scrape_news(self,):\n",
    "        \"\"\"\n",
    "        Scrapes Tesla news from the given Yahoo Finance URL.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "            A pandas DataFrame with columns 'title', 'summary', and 'date'.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            paper = newspaper.build(self.url)\n",
    "            articles = paper.articles\n",
    "        except:\n",
    "            print(\"Invalid Link!\")\n",
    "\n",
    "        data = []\n",
    "        for article in articles:\n",
    "            try:   \n",
    "                article.download()\n",
    "                article.parse()\n",
    "                text = article.text\n",
    "                summary = summarize(text)\n",
    "            except:\n",
    "                text= \"\"\n",
    "                summary = \"\"\n",
    "                self.error_counter += 1\n",
    "\n",
    "            data.append({'title': article.title,\n",
    "                        'text': text,\n",
    "                        'url': article.url,\n",
    "                        'summary': summary})\n",
    "\n",
    "        self.df = pd.DataFrame(data)\n",
    "        return self.df\n",
    "    \n",
    "    def saver(self, name, mode='w', dataFrame=None):\n",
    "        \"\"\"\n",
    "        saves the data to a file if called\n",
    "\n",
    "        Args:\n",
    "            name: what the name of the file should be.\n",
    "            dataFrame: the dataFrame that operation will be done on\n",
    "            mode: how you want to save? 'a', 'w', 'x'\n",
    "            - 'w', truncate the file first.\n",
    "            - 'x', exclusive creation, failing if the file already exists.\n",
    "            - 'a', append to the end of file if it exists.\n",
    "            \n",
    "\n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if not dataFrame:\n",
    "            self.df.to_csv(f'{name}.csv', index=False, mode=mode)\n",
    "        else:\n",
    "            dataFrame.to_csv(f'{name}.csv', index=False, mode=mode)\n",
    "        print('saved!')\n",
    "        \n",
    "\n",
    "def summarize(text):\n",
    "    if type(text) == type(None):\n",
    "        return \"\"\n",
    "    # Prepare the parser and summarizer\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizer = LsaSummarizer()\n",
    "\n",
    "    # Set the number of sentences you want in the summary\n",
    "    summary = summarizer(parser.document, 3)  # Summary with 3 sentences\n",
    "\n",
    "    sentenceV = \"\"\n",
    "    # Print the summary sentences\n",
    "    for sentence in summary:\n",
    "        sentenceV += str(sentence)\n",
    "    \n",
    "    return sentenceV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_date_no1(df):\n",
    "    '''\n",
    "    Method number one of getting dates. The current method built into the code. \n",
    "        has some error. \n",
    "    \n",
    "    Args:\n",
    "        df: the data Frame.\n",
    "    \n",
    "    Returns:\n",
    "        dataFrame. \n",
    "    \n",
    "    '''\n",
    "    error_counter = 0\n",
    "    \n",
    "    for i, url in enumerate(df['url']):\n",
    "        try:\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            df.at[i, 'date'] = article.publish_date\n",
    "        except Exception as e:\n",
    "            df.at[i, 'text'] = None\n",
    "            df.at[i, 'date'] = None\n",
    "            error_counter += 1\n",
    "            # print('error occured', e)\n",
    "    print(f\"Error Count: {error_counter}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_date_no2(df, url):\n",
    "    '''\n",
    "    Method number two of getting dates for different urls.\n",
    "\n",
    "    Args:\n",
    "        df: pandas dataframe. the dataFrame that needs to be adjusted\n",
    "        url: the url that the dataFrame belongs to.\n",
    "    \n",
    "    Returns:\n",
    "        the dataFrame back\n",
    "    '''\n",
    "    r = requests.get(url)\n",
    "    soup = bs(r.text, 'html.parser')\n",
    "\n",
    "    dates = soup.select('div.mt-1.text-sm.text-faded.sm\\\\:order-1.sm\\\\:mt-0')\n",
    "\n",
    "    all_dates = []\n",
    "\n",
    "    for date in dates:\n",
    "        all_dates.append(date['title'])\n",
    "    \n",
    "    df[\"date\"] = pd.Series(all_dates)\n",
    "    print(f\"Unsuccessful operations: {df[df['date'].apply(type) == type(np.nan)].shape[0]}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_sentiment_analysis(df):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"yiyanghkust/finbert-tone\", num_labels=3)\n",
    "\n",
    "    X = df['summary'].to_list()\n",
    "    labels = {0:'neutral', 1:'positive',2:'negative'}\n",
    "\n",
    "    sent_val = list()\n",
    "    for x in X:\n",
    "        inputs = tokenizer(x, return_tensors=\"pt\", padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs).logits\n",
    "    \n",
    "        probs = torch.nn.functional.softmax(outputs, dim=-1)\n",
    "        val = labels[np.argmax(outputs.detach().numpy())]\n",
    "        \n",
    "        score_dict = {\n",
    "            'neutral': probs[0][0].item(),\n",
    "            'positive': probs[0][1].item(),\n",
    "            'negative': probs[0][2].item()\n",
    "        }\n",
    "        \n",
    "        sent_val.append(score_dict)\n",
    "        \n",
    "    neural_list = []\n",
    "    positive_list = []\n",
    "    negative_list = []\n",
    "\n",
    "    for item in sent_val:\n",
    "        neural_list.append(item['neutral'])\n",
    "        positive_list.append(item['positive'])\n",
    "        negative_list.append(item['negative'])\n",
    "\n",
    "    df['neutral'] = neural_list\n",
    "    df['positive'] = positive_list\n",
    "    df['negative'] = negative_list\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_on_berted_df(df):\n",
    "    try: \n",
    "        positives = df['positive'].mean()\n",
    "        negative = df['negative'].mean()\n",
    "        neutral = df['neutral'].mean()\n",
    "        return f\"positive: {positives}, negative: {negative}, neutral: {neutral}\"\n",
    "    except:\n",
    "        return f\"You have to run it through bert_sentiment_analysis() first\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class do_everything():\n",
    "    '''\n",
    "    Does everything you need to do.\n",
    "\n",
    "    Args:\n",
    "        url: String. The url\n",
    "        fileName= the file name of the csv that will be saved to\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, url, fileName='project') -> None:\n",
    "        self.url = url\n",
    "        self.your_scraper = scraper(url=url)\n",
    "        #scrape the url\n",
    "        self.your_scraper.scrape_news()\n",
    "        self.df = self.your_scraper.df\n",
    "        # print(self.your_scraper.df.head())\n",
    "        print(self.df.head())\n",
    "        #get the date method one\n",
    "        self.df_date_1 = getting_date_no1(self.df)\n",
    "\n",
    "        #get the date method two\n",
    "        self.df_date_2 = getting_date_no2(self.df, url)\n",
    "\n",
    "        #analyse with finbert:\n",
    "        self.analysed = bert_sentiment_analysis(self.df)\n",
    "\n",
    "        self.anlaysed_str = analysis_on_berted_df(self.analysed)\n",
    "\n",
    "        #save to csv:\n",
    "        self.your_scraper.saver(name=fileName,mode='w', dataFrame=self.analysed)\n",
    "\n",
    "        print(self.anlaysed_str)\n",
    "        print(self.analysed.head())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'url'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m msft_news \u001b[38;5;241m=\u001b[39m \u001b[43mdo_everything\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://stockanalysis.com/stocks/msft/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfileName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmicrosft_news\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 19\u001b[0m, in \u001b[0;36mdo_everything.__init__\u001b[1;34m(self, url, fileName)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#get the date method one\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_date_1 \u001b[38;5;241m=\u001b[39m \u001b[43mgetting_date_no1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#get the date method two\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_date_2 \u001b[38;5;241m=\u001b[39m getting_date_no2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf, url)\n",
      "Cell \u001b[1;32mIn[3], line 15\u001b[0m, in \u001b[0;36mgetting_date_no1\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mMethod number one of getting dates. The current method built into the code. \u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    has some error. \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     13\u001b[0m error_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, url \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m         article \u001b[38;5;241m=\u001b[39m Article(url)\n",
      "File \u001b[1;32mc:\\Users\\mamin\\Desktop\\StockMl\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\mamin\\Desktop\\StockMl\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'url'"
     ]
    }
   ],
   "source": [
    "msft_news = do_everything(url='https://stockanalysis.com/stocks/msft/', fileName='microsft_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://stockanalysis.com/stocks/aapl/'\n",
    "your_scraper = scraper(url=url)\n",
    "your_scraper.scrape_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import newspaper\n",
    "paper = newspaper.build('https://stockanalysis.com/stocks/aapl/')\n",
    "print(len(paper.articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_number_two_scraping():\n",
    "    r = requests.get(url)\n",
    "    soup = bs(r.text, 'html.parser')\n",
    "    myBaddies = soup.select('gap-4.border-gray-300.bg-white.p-4 shadow.last\\\\:pb-1.last\\\\:shadow-none.dark\\\\:border-dark-600.dark\\\\:bg-dark-800.sm\\\\:border-b.sm\\\\:px-0.sm\\\\:shadow-none.sm\\\\:last\\\\:border-b-0.lg\\\\:gap-5.sm\\\\:grid.sm\\\\:grid-cols-news.sm\\\\:py-6')\n",
    "    links = soup.select('a.text-default.hover\\\\:text-blue-brand_sharp.dark\\\\:text-neutral-300.dark\\\\:hover\\\\:text-blue-darklink')\n",
    "\n",
    "    # Extract href attributes\n",
    "    hrefs = [link['href'] for link in links if 'href' in link.attrs]\n",
    "    print(hrefs)\n",
    "    print(myBaddies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: name 'article' is not defined\n",
      "An error occurred: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.wsj.com/articles/apple-offers-100-million-investment-in-indonesia-to-lift-iphone-16-ban-67462dba?mod=rss_Technology on URL https://www.wsj.com/articles/apple-offers-100-million-investment-in-indonesia-to-lift-iphone-16-ban-67462dba?mod=rss_Technology\n",
      "An error occurred: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.barrons.com/articles/qualcomm-stock-apple-amd-intel-chips-7c82508f on URL https://www.barrons.com/articles/qualcomm-stock-apple-amd-intel-chips-7c82508f\n",
      "An error occurred: Article `download()` failed with 401 Client Error: HTTP Forbidden for url: https://www.reuters.com/technology/apple-sends-100-mln-investment-proposal-build-plant-indonesia-2024-11-20/ on URL https://www.reuters.com/technology/apple-sends-100-mln-investment-proposal-build-plant-indonesia-2024-11-20/\n",
      "An error occurred: Article `download()` failed with 401 Client Error: HTTP Forbidden for url: https://www.reuters.com/technology/apple-urge-judge-end-us-smartphone-monopoly-case-2024-11-20/ on URL https://www.reuters.com/technology/apple-urge-judge-end-us-smartphone-monopoly-case-2024-11-20/\n",
      "An error occurred: Article `download()` failed with 401 Client Error: HTTP Forbidden for url: https://www.reuters.com/technology/apple-deletes-us-funded-rferl-news-app-russian-app-store-news-outlet-says-2024-11-15/ on URL https://www.reuters.com/technology/apple-deletes-us-funded-rferl-news-app-russian-app-store-news-outlet-says-2024-11-15/\n",
      "An error occurred: Article `download()` failed with 403 Client Error: Forbidden for url: https://seekingalpha.com/article/4737670-magnificent-seven-are-alive-and-well-but-rest-is-inevitable?source=affiliate_program:stockanalysis.com&utm_medium=affiliate&utm_source=stockanalysis.com&affid=858&oid=16&transaction=d3c8c0d03e9a4185a02af739d700256e on URL https://stockanalysis.com/out/news?url=https://seekingalpha.com/article/4737670-magnificent-seven-are-alive-and-well-but-rest-is-inevitable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nov 21, 2024, 9:00 AM EST</td>\n",
       "      <td>https://www.prnewswire.com/news-releases/john-...</td>\n",
       "      <td>John Hancock Adds New Apple Watch Series 10 to...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nov 21, 2024, 8:58 AM EST</td>\n",
       "      <td>https://www.cnbc.com/2024/11/21/cfpb-expands-o...</td>\n",
       "      <td>CFPB expands oversight of digital payments ser...</td>\n",
       "      <td>BOSTON, Nov. 21, 2024 /PRNewswire/ - Today, Jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nov 21, 2024, 6:20 AM EST</td>\n",
       "      <td>https://techxplore.com/news/2024-11-apple-urge...</td>\n",
       "      <td>Apple urges judge to dismiss US antitrust laws...</td>\n",
       "      <td>Rohit Chopra, director of the CFPB, testifies ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nov 21, 2024, 1:25 AM EST</td>\n",
       "      <td>https://www.wsj.com/articles/apple-offers-100-...</td>\n",
       "      <td>Apple Offers $100 Million Investment in Indone...</td>\n",
       "      <td>This article has been reviewed according to Sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nov 20, 2024, 12:49 PM EST</td>\n",
       "      <td>https://www.pymnts.com/apple/2024/apple-seeks-...</td>\n",
       "      <td>Apple Seeks Dismissal of U.S. Smartphone Monop...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        title  \\\n",
       "0   Nov 21, 2024, 9:00 AM EST   \n",
       "1   Nov 21, 2024, 8:58 AM EST   \n",
       "2   Nov 21, 2024, 6:20 AM EST   \n",
       "3   Nov 21, 2024, 1:25 AM EST   \n",
       "4  Nov 20, 2024, 12:49 PM EST   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.prnewswire.com/news-releases/john-...   \n",
       "1  https://www.cnbc.com/2024/11/21/cfpb-expands-o...   \n",
       "2  https://techxplore.com/news/2024-11-apple-urge...   \n",
       "3  https://www.wsj.com/articles/apple-offers-100-...   \n",
       "4  https://www.pymnts.com/apple/2024/apple-seeks-...   \n",
       "\n",
       "                                            headline  \\\n",
       "0  John Hancock Adds New Apple Watch Series 10 to...   \n",
       "1  CFPB expands oversight of digital payments ser...   \n",
       "2  Apple urges judge to dismiss US antitrust laws...   \n",
       "3  Apple Offers $100 Million Investment in Indone...   \n",
       "4  Apple Seeks Dismissal of U.S. Smartphone Monop...   \n",
       "\n",
       "                                                text  \n",
       "0                                                     \n",
       "1  BOSTON, Nov. 21, 2024 /PRNewswire/ - Today, Jo...  \n",
       "2  Rohit Chopra, director of the CFPB, testifies ...  \n",
       "3  This article has been reviewed according to Sc...  \n",
       "4                                                     "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get(url)\n",
    "soup = bs(r.text, 'html.parser')\n",
    "containers = soup.select('div.gap-4.border-gray-300.bg-white.p-4.shadow.last\\\\:pb-1.last\\\\:shadow-none.dark\\\\:border-dark-600.dark\\\\:bg-dark-800.sm\\\\:border-b.sm\\\\:px-0.sm\\\\:shadow-none.sm\\\\:last\\\\:border-b-0.lg\\\\:gap-5.sm\\\\:grid.sm\\\\:grid-cols-news.sm\\\\:py-6')\n",
    "\n",
    "# Extract href attributes\n",
    "data = []\n",
    "for container in containers:\n",
    "    title = container.select('div.mt-1.text-sm.text-faded.sm\\\\:order-1.sm\\\\:mt-0')[0]['title']\n",
    "    aTag = container.select('a.text-default.hover\\\\:text-blue-brand_sharp.dark\\\\:text-neutral-300.dark\\\\:hover\\\\:text-blue-darklink')[0]\n",
    "    link = aTag['href']\n",
    "    headline = aTag.text\n",
    "    article = Article(link)\n",
    "    try:\n",
    "        # Download the article content\n",
    "        article.download()\n",
    "\n",
    "        # Parse the article to extract text and metadata\n",
    "        article.parse()\n",
    "\n",
    "        # Access the article text\n",
    "        text = article.text\n",
    "        summary = summarize(text)\n",
    "    except Exception as e:\n",
    "        text = \"\"\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    data.append({\n",
    "        'title': title,\n",
    "        'link': link,\n",
    "        'headline': headline,\n",
    "        'text': text,\n",
    "        'summary': summary\n",
    "    })\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "scrapeResults = pd.DataFrame(data)\n",
    "scrapeResults.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
